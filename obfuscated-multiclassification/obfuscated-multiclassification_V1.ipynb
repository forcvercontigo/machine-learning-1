{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Concatenate, Dense, Dropout, Softmax\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data can be downloaded from https://www.kaggle.com/alaeddineayadi/obfuscated-multiclassification\n",
    "with open('./data/xtrain_obfuscated.txt') as fp:\n",
    "    data = fp.read().split('\\n')\n",
    "with open('./data/ytrain.txt') as fp:\n",
    "    label = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char = 26\n",
    "max_len = 0\n",
    "for sent in data:\n",
    "    max_len = max(max_len, len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding for labels\n",
    "for i in range(len(label)):\n",
    "    label[i] = int(label[i])\n",
    "label = np.asarray(label)\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "label = label.reshape(-1,1)\n",
    "y_train_data = ohe.fit_transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot feature generation\n",
    "# MaxLen = 452. \n",
    "# Other sentences are 0 padded\n",
    "X_train = []\n",
    "for i in range(len(data)):\n",
    "    temp = np.zeros((max_len, max_char))\n",
    "    for j in range(len(data[i])):\n",
    "        temp[j][ord(data[i][j])-97] = 1\n",
    "    X_train.append(temp)\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1], X_train.shape[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) CNN in text data can be used to capture local/temporal dependencies in data.\n",
    "# eg, in this case i am performing 8 different CNN operations over same data, with different convolution size\n",
    "# 2) Here every convolution size means, CNN is trying to capture useful pattern in data in that size frame.\n",
    "# eg, with convolution size 5, CNN will check for all 5-grams(chars in this case) in sentence and try to learn \n",
    "# useful pattern in data\n",
    "# 3) In case of text data kernal_size = pattern_size X feature_size. Since we need to consider whole char a \n",
    "# the same time, that why feature_size is important factor here\n",
    "\n",
    "convSize_1 = 3\n",
    "convSize_2 = 4\n",
    "convSize_3 = 5\n",
    "convSize_4 = 6\n",
    "convSize_5 = 7\n",
    "convSize_6 = 8\n",
    "convSize_7 = 9\n",
    "convSize_8 = 10\n",
    "\n",
    "# Convolution with different kernal sizes\n",
    "inputLayer = Input(shape=(X_train[0].shape[0],X_train[0].shape[1],1))\n",
    "convLayer_1 = Conv2D(filters=128, kernel_size=(convSize_1,X_train[0].shape[1]))(inputLayer)\n",
    "convLayer_2 = Conv2D(filters=128, kernel_size=(convSize_2,X_train[0].shape[1]))(inputLayer)\n",
    "convLayer_3 = Conv2D(filters=128, kernel_size=(convSize_3,X_train[0].shape[1]))(inputLayer)\n",
    "convLayer_4 = Conv2D(filters=128, kernel_size=(convSize_4,X_train[0].shape[1]))(inputLayer)\n",
    "convLayer_5 = Conv2D(filters=128, kernel_size=(convSize_5,X_train[0].shape[1]))(inputLayer)\n",
    "convLayer_6 = Conv2D(filters=128, kernel_size=(convSize_6,X_train[0].shape[1]))(inputLayer)\n",
    "convLayer_7 = Conv2D(filters=128, kernel_size=(convSize_7,X_train[0].shape[1]))(inputLayer)\n",
    "convLayer_8 = Conv2D(filters=128, kernel_size=(convSize_8,X_train[0].shape[1]))(inputLayer)\n",
    "\n",
    "#Dropout to prevent overfitting\n",
    "dropout_1 = Dropout(0.5)(convLayer_1)\n",
    "dropout_2 = Dropout(0.5)(convLayer_2)\n",
    "dropout_3 = Dropout(0.5)(convLayer_3)\n",
    "dropout_4 = Dropout(0.5)(convLayer_4)\n",
    "dropout_5 = Dropout(0.5)(convLayer_5)\n",
    "dropout_6 = Dropout(0.5)(convLayer_6)\n",
    "dropout_7 = Dropout(0.5)(convLayer_7)\n",
    "dropout_8 = Dropout(0.5)(convLayer_8)\n",
    "\n",
    "#Maxpool within the features\n",
    "maxPool_1 = MaxPooling2D(pool_size=(max_len-convSize_1+1, 1))(dropout_1)\n",
    "maxPool_2 = MaxPooling2D(pool_size=(max_len-convSize_2+1, 1))(dropout_2)\n",
    "maxPool_3 = MaxPooling2D(pool_size=(max_len-convSize_3+1, 1))(dropout_3)\n",
    "maxPool_4 = MaxPooling2D(pool_size=(max_len-convSize_4+1, 1))(dropout_4)\n",
    "maxPool_5 = MaxPooling2D(pool_size=(max_len-convSize_5+1, 1))(dropout_5)\n",
    "maxPool_6 = MaxPooling2D(pool_size=(max_len-convSize_6+1, 1))(dropout_6)\n",
    "maxPool_7 = MaxPooling2D(pool_size=(max_len-convSize_7+1, 1))(dropout_7)\n",
    "maxPool_8 = MaxPooling2D(pool_size=(max_len-convSize_8+1, 1))(dropout_8)\n",
    "\n",
    "#Flatten all the data from CNN model\n",
    "flatten_1 = Flatten()(maxPool_1)\n",
    "flatten_2 = Flatten()(maxPool_2)\n",
    "flatten_3 = Flatten()(maxPool_3)\n",
    "flatten_4 = Flatten()(maxPool_4)\n",
    "flatten_5 = Flatten()(maxPool_5)\n",
    "flatten_6 = Flatten()(maxPool_6)\n",
    "flatten_7 = Flatten()(maxPool_7)\n",
    "flatten_8 = Flatten()(maxPool_8)\n",
    "\n",
    "#Merge all the 8 layers data\n",
    "mergedLayer = Concatenate(axis=1)([flatten_1, flatten_2, flatten_3, flatten_4, flatten_5, flatten_6, flatten_7, flatten_8])\n",
    "\n",
    "#Dense layers and so on\n",
    "dense_1 = Dense(1024, activation='relu')(mergedLayer)\n",
    "dropout_9 = Dropout(0.5)(dense_1)\n",
    "dense_2 = Dense(128, activation='relu')(dropout_9)\n",
    "dropout_10 = Dropout(0.5)(dense_2)\n",
    "\n",
    "result = Dense(12, activation='softmax')(dropout_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 452, 26, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 450, 1, 128)  10112       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 449, 1, 128)  13440       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 448, 1, 128)  16768       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 447, 1, 128)  20096       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 446, 1, 128)  23424       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 445, 1, 128)  26752       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 444, 1, 128)  30080       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 443, 1, 128)  33408       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 450, 1, 128)  0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 449, 1, 128)  0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 448, 1, 128)  0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 447, 1, 128)  0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 446, 1, 128)  0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 445, 1, 128)  0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 444, 1, 128)  0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 443, 1, 128)  0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 1, 1, 128)    0           dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 128)          0           max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 128)          0           max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 128)          0           max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 128)          0           max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 128)          0           max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 128)          0           max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 128)          0           max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 128)          0           max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1024)         0           flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "                                                                 flatten_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         1049600     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          131200      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 128)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 12)           1548        dropout_30[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,356,428\n",
      "Trainable params: 1,356,428\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputLayer, result)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "tensorboard = TensorBoard(log_dir='./logs/{}'.format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26010 samples, validate on 6503 samples\n",
      "Epoch 1/50\n",
      "26010/26010 [==============================] - 21s 824us/step - loss: 2.1312 - acc: 0.2508 - val_loss: 1.5536 - val_acc: 0.5156\n",
      "Epoch 2/50\n",
      "26010/26010 [==============================] - 21s 790us/step - loss: 1.2863 - acc: 0.5531 - val_loss: 1.0863 - val_acc: 0.6425\n",
      "Epoch 3/50\n",
      "26010/26010 [==============================] - 20s 787us/step - loss: 1.0084 - acc: 0.6569 - val_loss: 0.8650 - val_acc: 0.7334\n",
      "Epoch 4/50\n",
      "26010/26010 [==============================] - 21s 791us/step - loss: 0.8495 - acc: 0.7201 - val_loss: 0.7679 - val_acc: 0.7680\n",
      "Epoch 5/50\n",
      "26010/26010 [==============================] - 21s 794us/step - loss: 0.7362 - acc: 0.7575 - val_loss: 0.6891 - val_acc: 0.7827\n",
      "Epoch 6/50\n",
      "26010/26010 [==============================] - 21s 792us/step - loss: 0.6719 - acc: 0.7792 - val_loss: 0.6419 - val_acc: 0.7987\n",
      "Epoch 7/50\n",
      "26010/26010 [==============================] - 21s 793us/step - loss: 0.6268 - acc: 0.7957 - val_loss: 0.6086 - val_acc: 0.8119\n",
      "Epoch 8/50\n",
      "26010/26010 [==============================] - 21s 797us/step - loss: 0.5649 - acc: 0.8174 - val_loss: 0.5985 - val_acc: 0.8132\n",
      "Epoch 9/50\n",
      "26010/26010 [==============================] - 21s 796us/step - loss: 0.5332 - acc: 0.8279 - val_loss: 0.5599 - val_acc: 0.8278\n",
      "Epoch 10/50\n",
      "26010/26010 [==============================] - 21s 788us/step - loss: 0.5051 - acc: 0.8364 - val_loss: 0.5296 - val_acc: 0.8322\n",
      "Epoch 11/50\n",
      "26010/26010 [==============================] - 20s 787us/step - loss: 0.4779 - acc: 0.8444 - val_loss: 0.5367 - val_acc: 0.8290\n",
      "Epoch 12/50\n",
      "26010/26010 [==============================] - 21s 792us/step - loss: 0.4574 - acc: 0.8521 - val_loss: 0.5497 - val_acc: 0.8238\n",
      "Epoch 13/50\n",
      "26010/26010 [==============================] - 21s 792us/step - loss: 0.4395 - acc: 0.8585 - val_loss: 0.5450 - val_acc: 0.8299\n",
      "Epoch 14/50\n",
      "26010/26010 [==============================] - 21s 791us/step - loss: 0.4212 - acc: 0.8655 - val_loss: 0.4878 - val_acc: 0.8462\n",
      "Epoch 15/50\n",
      "26010/26010 [==============================] - 21s 792us/step - loss: 0.3936 - acc: 0.8717 - val_loss: 0.5010 - val_acc: 0.8408\n",
      "Epoch 16/50\n",
      "26010/26010 [==============================] - 21s 793us/step - loss: 0.3977 - acc: 0.8736 - val_loss: 0.5172 - val_acc: 0.8381\n",
      "Epoch 17/50\n",
      "26010/26010 [==============================] - 21s 796us/step - loss: 0.3722 - acc: 0.8785 - val_loss: 0.5177 - val_acc: 0.8365\n",
      "Epoch 18/50\n",
      "26010/26010 [==============================] - 21s 794us/step - loss: 0.3629 - acc: 0.8817 - val_loss: 0.4838 - val_acc: 0.8415\n",
      "Epoch 19/50\n",
      "26010/26010 [==============================] - 21s 793us/step - loss: 0.3577 - acc: 0.8843 - val_loss: 0.4787 - val_acc: 0.8490\n",
      "Epoch 20/50\n",
      "26010/26010 [==============================] - 21s 794us/step - loss: 0.3507 - acc: 0.8867 - val_loss: 0.5249 - val_acc: 0.8375\n",
      "Epoch 21/50\n",
      "26010/26010 [==============================] - 21s 791us/step - loss: 0.3458 - acc: 0.8899 - val_loss: 0.4791 - val_acc: 0.8471\n",
      "Epoch 22/50\n",
      "26010/26010 [==============================] - 21s 792us/step - loss: 0.3217 - acc: 0.8982 - val_loss: 0.4915 - val_acc: 0.8459\n",
      "Epoch 23/50\n",
      "26010/26010 [==============================] - 20s 788us/step - loss: 0.3174 - acc: 0.8976 - val_loss: 0.4694 - val_acc: 0.8547\n",
      "Epoch 24/50\n",
      "26010/26010 [==============================] - 21s 795us/step - loss: 0.3130 - acc: 0.8999 - val_loss: 0.4768 - val_acc: 0.8515\n",
      "Epoch 25/50\n",
      "26010/26010 [==============================] - 21s 788us/step - loss: 0.3049 - acc: 0.9023 - val_loss: 0.4736 - val_acc: 0.8545\n",
      "Epoch 26/50\n",
      "26010/26010 [==============================] - 21s 789us/step - loss: 0.2957 - acc: 0.9057 - val_loss: 0.4718 - val_acc: 0.8591\n",
      "Epoch 27/50\n",
      "26010/26010 [==============================] - 20s 788us/step - loss: 0.2932 - acc: 0.9046 - val_loss: 0.4646 - val_acc: 0.8567\n",
      "Epoch 28/50\n",
      "26010/26010 [==============================] - 21s 793us/step - loss: 0.2884 - acc: 0.9070 - val_loss: 0.4807 - val_acc: 0.8584\n",
      "Epoch 29/50\n",
      "26010/26010 [==============================] - 21s 790us/step - loss: 0.2775 - acc: 0.9097 - val_loss: 0.4752 - val_acc: 0.8555\n",
      "Epoch 30/50\n",
      "26010/26010 [==============================] - 21s 791us/step - loss: 0.2749 - acc: 0.9116 - val_loss: 0.4958 - val_acc: 0.8491\n",
      "Epoch 31/50\n",
      "26010/26010 [==============================] - 21s 792us/step - loss: 0.2669 - acc: 0.9133 - val_loss: 0.5097 - val_acc: 0.8505\n",
      "Epoch 32/50\n",
      "26010/26010 [==============================] - 21s 796us/step - loss: 0.2551 - acc: 0.9166 - val_loss: 0.4812 - val_acc: 0.8553\n",
      "Epoch 33/50\n",
      "26010/26010 [==============================] - 21s 790us/step - loss: 0.2633 - acc: 0.9141 - val_loss: 0.5126 - val_acc: 0.8445\n",
      "Epoch 34/50\n",
      "26010/26010 [==============================] - 20s 786us/step - loss: 0.2541 - acc: 0.9181 - val_loss: 0.5118 - val_acc: 0.8476\n",
      "Epoch 35/50\n",
      "26010/26010 [==============================] - 21s 800us/step - loss: 0.2556 - acc: 0.9185 - val_loss: 0.5022 - val_acc: 0.8478\n",
      "Epoch 36/50\n",
      "26010/26010 [==============================] - 21s 791us/step - loss: 0.2416 - acc: 0.9214 - val_loss: 0.4963 - val_acc: 0.8521\n",
      "Epoch 37/50\n",
      "26010/26010 [==============================] - 21s 791us/step - loss: 0.2478 - acc: 0.9170 - val_loss: 0.5052 - val_acc: 0.8518\n",
      "Epoch 38/50\n",
      "26010/26010 [==============================] - 21s 790us/step - loss: 0.2422 - acc: 0.9214 - val_loss: 0.5141 - val_acc: 0.8531\n",
      "Epoch 39/50\n",
      "26010/26010 [==============================] - 21s 795us/step - loss: 0.2360 - acc: 0.9246 - val_loss: 0.5232 - val_acc: 0.8516\n",
      "Epoch 40/50\n",
      "26010/26010 [==============================] - 21s 796us/step - loss: 0.2310 - acc: 0.9258 - val_loss: 0.5501 - val_acc: 0.8501\n",
      "Epoch 41/50\n",
      "26010/26010 [==============================] - 21s 795us/step - loss: 0.2211 - acc: 0.9289 - val_loss: 0.5040 - val_acc: 0.8556\n",
      "Epoch 42/50\n",
      "26010/26010 [==============================] - 21s 790us/step - loss: 0.2187 - acc: 0.9287 - val_loss: 0.5247 - val_acc: 0.8556\n",
      "Epoch 43/50\n",
      "26010/26010 [==============================] - 21s 792us/step - loss: 0.2270 - acc: 0.9276 - val_loss: 0.5176 - val_acc: 0.8538\n",
      "Epoch 44/50\n",
      "26010/26010 [==============================] - 21s 797us/step - loss: 0.2183 - acc: 0.9291 - val_loss: 0.4931 - val_acc: 0.8590\n",
      "Epoch 45/50\n",
      "26010/26010 [==============================] - 21s 794us/step - loss: 0.2132 - acc: 0.9301 - val_loss: 0.5348 - val_acc: 0.8556\n",
      "Epoch 46/50\n",
      "26010/26010 [==============================] - 21s 793us/step - loss: 0.2104 - acc: 0.9323 - val_loss: 0.5343 - val_acc: 0.8547\n",
      "Epoch 47/50\n",
      "26010/26010 [==============================] - 21s 802us/step - loss: 0.2049 - acc: 0.9333 - val_loss: 0.5089 - val_acc: 0.8568\n",
      "Epoch 48/50\n",
      "26010/26010 [==============================] - 21s 803us/step - loss: 0.2073 - acc: 0.9334 - val_loss: 0.5425 - val_acc: 0.8556\n",
      "Epoch 49/50\n",
      "26010/26010 [==============================] - 21s 804us/step - loss: 0.2054 - acc: 0.9336 - val_loss: 0.5279 - val_acc: 0.8636\n",
      "Epoch 50/50\n",
      "26010/26010 [==============================] - 21s 800us/step - loss: 0.2037 - acc: 0.9341 - val_loss: 0.5296 - val_acc: 0.8559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5bbc3e7518>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_data, \n",
    "        batch_size=128, \n",
    "        verbose=1, epochs=50, \n",
    "        shuffle=True, \n",
    "        validation_split=0.2,\n",
    "        callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('8_layer_conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
